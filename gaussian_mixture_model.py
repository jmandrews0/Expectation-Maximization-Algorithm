import plot_data_and_gaussians as Pimport numpy as npimport matplotlib.pyplot as pltdef gaussian(mu, cov, p):    diff = mu - p    return np.exp(-0.5*np.matmul(diff,np.linalg.inv(cov)).dot(diff)) / np.sqrt((2*np.pi)**2 * np.linalg.det(cov))def logLikelihood(D, W, alpha, mu, cov):    likelihood = 0    for i in range(len(D)):        sum1 = 0;        for k in range(len(mu)):            sum1 += gaussian(mu[k], cov[k], D[i]) * alpha[k]        likelihood += np.log(sum1)    return likelihooddef EM(D, K, init_method, epsilon, plotflag, RSEED=123):    likelihoodData = []    # initialize....    N = len(D)    W = np.zeros((N, K))    alpha = np.ones(K)*(1/K)    Nk = np.zeros(K)        mu = np.zeros((K,2))    for i in range(K):        mu[i] = D[np.random.randint(N)]    cov = np.zeros((K,2,2))    for i in range(K):        cov[i] = np.eye(2)            orig_mu = np.array(mu)    orig_cov = np.array(cov)        likelihoods = np.zeros(2);    converged = False;    while not converged:        # perform E-step        for i in range(N):            for k in range(K):                W[i,k] = gaussian(mu[k], cov[k], D[i]) * alpha[k]            W[i] = W[i] / sum(W[i])            # perform M-step        for k in range(K):            Nk[k] = np.sum(W[:,k])            alpha[k] = Nk[k] / N                for k in range(K):            mu[k] = np.matmul(D.transpose(), W[:,k]) / Nk[k]                    for k in range(K):            sum1 = np.zeros((2,2))            for i in range(N):                #print(D[i]-mu[k])                sum1 += W[i,k] * np.diag((D[i]-mu[k])*(D[i]-mu[k])+1e-4)#W[i,k] * (D[i]-mu[k]) * (D[i]-mu[k]).reshape(mu[k].shape[0],1)            sum1 /= Nk[k]            cov[k] = sum1                # avoid singular solutions        for k in range(K):            for i in range(len(cov[k])):                if cov[k,i,i] < 1e-4:                    cov[k,i,i] = 1e-4                # compute log-likelihood and print to screen        likelihoods[1] = likelihoods[0]        likelihoods[0] = logLikelihood(D, W, alpha, mu, cov)        likelihoodData.append(likelihoods[0])                # check for convergence        if np.abs(likelihoods[1] - likelihoods[0]) < epsilon:            converged = True                    #P.plot_data_and_gaussians(D, mu, cov, "Initial Parameter Values")        #input()        #return gparams, memberships    return orig_mu, orig_cov, mu, cov, np.array(likelihoodData)    def EM_N_times(D, K, N, init_method, epsilon, plotflag):        mu1, cov1, mu2, cov2, plot = gaussian_mixture(D, K, init_method, epsilon, plotflag)    for i in range(N-1):        #print("iteration: ", i+1)        n_mu1, n_mu2, n_cov1, n_cov2, n_plot = gaussian_mixture(D, K, init_method, epsilon, plotflag)        if n_plot[-1] > plot[-1]:            mu1 = np.array(n_mu1)            mu2 = np.array(n_mu2)            cov1 = np.array(n_cov1)            cov2 = np.array(n_cov2)            plot = np.array(n_plot)            #print("    log-likelihood: ", plot[-1])        return mu1, mu2, cov1, cov2, plotif __name__ == '__main__':    data = np.genfromtxt("dataset1.txt")    mu1, cov1, mu2, cov2, plot = EM(data, 2, 1, 0.01, 10, True)    P.plot_data_and_gaussians(data, mu2, cov2, "Initial Parameter Values")